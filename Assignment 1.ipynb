{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : cats \n",
      "Stemmed Word : cat \n",
      "Lemma : cat\n",
      "\n",
      "Word : trouble \n",
      "Stemmed Word : troubl \n",
      "Lemma : trouble\n",
      "\n",
      "Word : troubling \n",
      "Stemmed Word : troubl \n",
      "Lemma : troubling\n",
      "\n",
      "Word : troubled \n",
      "Stemmed Word : troubl \n",
      "Lemma : troubled\n",
      "\n",
      "Word : having \n",
      "Stemmed Word : have \n",
      "Lemma : having\n",
      "\n",
      "Word : corriendo \n",
      "Stemmed Word : corriendo \n",
      "Lemma : corriendo\n",
      "\n",
      "Word : at \n",
      "Stemmed Word : at \n",
      "Lemma : at\n",
      "\n",
      "Word : was \n",
      "Stemmed Word : wa \n",
      "Lemma : wa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find stem and lemma words for the given words\n",
    "# cats\n",
    "# trouble\n",
    "# troubling\n",
    "# troubled\n",
    "# having\n",
    "# Corriendo\n",
    "# at\n",
    "# was\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"cats\", \"trouble\", \"troubling\", \"troubled\", \"having\", \"corriendo\", \"at\", \"was\"]\n",
    "for word in words:\n",
    "    print('Word :', word, \"\\nStemmed Word :\", stemmer.stem(word), '\\nLemma :', lemmatizer.lemmatize(word), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Stem</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processing</td>\n",
       "      <td>1</td>\n",
       "      <td>process</td>\n",
       "      <td>processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>1</td>\n",
       "      <td>book</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lives</td>\n",
       "      <td>1</td>\n",
       "      <td>live</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time</td>\n",
       "      <td>1</td>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>applications</td>\n",
       "      <td>1</td>\n",
       "      <td>applic</td>\n",
       "      <td>application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>impact</td>\n",
       "      <td>1</td>\n",
       "      <td>impact</td>\n",
       "      <td>impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>useful</td>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "      <td>useful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>whether</td>\n",
       "      <td>1</td>\n",
       "      <td>whether</td>\n",
       "      <td>whether</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>our</td>\n",
       "      <td>1</td>\n",
       "      <td>our</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>through</td>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>those</td>\n",
       "      <td>1</td>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>they</td>\n",
       "      <td>1</td>\n",
       "      <td>they</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>articles</td>\n",
       "      <td>1</td>\n",
       "      <td>articl</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>media</td>\n",
       "      <td>1</td>\n",
       "      <td>media</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ever</td>\n",
       "      <td>1</td>\n",
       "      <td>ever</td>\n",
       "      <td>ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "      <td>huge</td>\n",
       "      <td>huge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>already</td>\n",
       "      <td>1</td>\n",
       "      <td>alreadi</td>\n",
       "      <td>already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>summarization</td>\n",
       "      <td>1</td>\n",
       "      <td>summar</td>\n",
       "      <td>summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>go</td>\n",
       "      <td>1</td>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>natural</td>\n",
       "      <td>1</td>\n",
       "      <td>natur</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>language</td>\n",
       "      <td>1</td>\n",
       "      <td>languag</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>decide</td>\n",
       "      <td>1</td>\n",
       "      <td>decid</td>\n",
       "      <td>decide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>documents</td>\n",
       "      <td>1</td>\n",
       "      <td>document</td>\n",
       "      <td>document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>has</td>\n",
       "      <td>1</td>\n",
       "      <td>ha</td>\n",
       "      <td>ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nlp</td>\n",
       "      <td>1</td>\n",
       "      <td>nlp</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>who</td>\n",
       "      <td>1</td>\n",
       "      <td>who</td>\n",
       "      <td>who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>thankfully</td>\n",
       "      <td>1</td>\n",
       "      <td>thank</td>\n",
       "      <td>thankfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>growing</td>\n",
       "      <td>2</td>\n",
       "      <td>grow</td>\n",
       "      <td>growing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>publishing</td>\n",
       "      <td>1</td>\n",
       "      <td>publish</td>\n",
       "      <td>publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>entire</td>\n",
       "      <td>1</td>\n",
       "      <td>entir</td>\n",
       "      <td>entire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>technology</td>\n",
       "      <td>1</td>\n",
       "      <td>technolog</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>which</td>\n",
       "      <td>1</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bound</td>\n",
       "      <td>1</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>digital</td>\n",
       "      <td>1</td>\n",
       "      <td>digit</td>\n",
       "      <td>digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>one</td>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>or</td>\n",
       "      <td>1</td>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>here</td>\n",
       "      <td>1</td>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word Frequency       Stem          Lemma\n",
       "0             and         1        and            and\n",
       "1      processing         1    process     processing\n",
       "2           books         1       book           book\n",
       "3           lives         1       live           life\n",
       "4            time         1       time           time\n",
       "5    applications         1     applic    application\n",
       "6          impact         1     impact         impact\n",
       "7          useful         1        use         useful\n",
       "8         whether         1    whether        whether\n",
       "9              to         3         to             to\n",
       "10            are         1        are            are\n",
       "11            our         1        our            our\n",
       "12        through         1    through        through\n",
       "13          those         1      those          those\n",
       "14           they         1       they           they\n",
       "15       articles         1     articl        article\n",
       "16          media         1      media         medium\n",
       "17           ever         1       ever           ever\n",
       "18            not         1        not            not\n",
       "19           huge         1       huge           huge\n",
       "20        already         1    alreadi        already\n",
       "21  summarization         1     summar  summarization\n",
       "22             go         1         go             go\n",
       "23        natural         1      natur        natural\n",
       "24       language         1    languag       language\n",
       "25         decide         1      decid         decide\n",
       "26      documents         1   document       document\n",
       "27            has         1         ha             ha\n",
       "28           text         1       text           text\n",
       "29            nlp         1        nlp            nlp\n",
       "30            who         1        who            who\n",
       "31     thankfully         1      thank     thankfully\n",
       "32        growing         2       grow        growing\n",
       "33           this         1        thi           this\n",
       "34             of         2         of             of\n",
       "35     publishing         1    publish     publishing\n",
       "36         entire         1      entir         entire\n",
       "37     technology         1  technolog     technology\n",
       "38          which         1      which          which\n",
       "39           with         1       with           with\n",
       "40          bound         1      bound          bound\n",
       "41        digital         1      digit        digital\n",
       "42             on         1         on             on\n",
       "43           have         1       have           have\n",
       "44            one         1        one            one\n",
       "45             or         1         or             or\n",
       "46           here         1       here           here"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find BoW for the given paragraph? And also find stem and lemma words?\n",
    "# Text Summarization is one of those applications of Natural Language\n",
    "# Processing (NLP) which is bound to have a huge impact on our lives.\n",
    "# With growing digital media and ever-growing publishing – who has the time\n",
    "# to go through entire articles / documents / books to decide whether they\n",
    "# are useful or not? Thankfully – this technology is already here.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def word_extraction(sentence):\n",
    "    ignore = ['a', \"the\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
    "text = word_extraction(text)\n",
    "data = pd.DataFrame(columns=['Word', 'Frequency', 'Stem', 'Lemma'])\n",
    "for word in set(text):\n",
    "    data.loc[len(data.index)] = [word, text.count(word), stemmer.stem(word), lemmatizer.lemmatize(word)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
